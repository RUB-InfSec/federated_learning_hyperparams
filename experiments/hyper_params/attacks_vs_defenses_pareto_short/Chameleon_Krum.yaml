resources:
  devices: "0"
  cpus_per_client: 1
  gpus_per_client: 0.1

# General setting
num_clients: 100
clients_per_round: 10
num_rounds: 205
partitioning: 'dirichlet'
partitioning_hyperparam: 0.9

# Strategy
strategy:
  name: 'fedavg'
  lr_scheduler:
    name: 'exp'
    gamma: 0.999

# Load trained model
load_benign_model:
  trained_rounds: 900
  experiment_descriptor: 'BenignTraining_0.1_decay'

# Task configuration
dataset: 'cifar10'
model: 'resnet20'
batch_size: 64
optimizer:
  name: 'sgd'
  local_lr: 0.1
  momentum: 0.9
  weight_decay: 0.0005
lr_scheduler:
  name:
  step_size: 2
  gamma: 0.8
local_epochs: 2

# Adversary configuration
attacker:
  name: 'chameleon'
  contrastive_local_epochs: 10
  contrastive_optimizer:
    local_lr: 0.005
    momentum: 0.9
    weight_decay: 0.0005
  contrastive_lr_scheduler:
    milestones: [3, 5, 7, 9]
    gamma: 0.1
  contrastive_poison_ratio: 0.1875
  beta: 2
  gamma: 1 # This is the model replacement scaling rate - apparently, they do only do a data poisoning attack
  split_layer: 'layer3'
  norm_bound: 5
adversarial_optimizer:
  name: 'sgd'
  local_lr: 0.001
  momentum: 0.9
  weight_decay: 0.005
adversarial_lr_scheduler:
  name: 'multi_step'
  milestones: [0.2, 0.8]
  gamma: 0.005
local_epochs_malicious_clients: 3
compromised_clients: 0.1

# Backdoor configuration
backdoor:
  type: 'artificial_trigger'
  source_class:
  target_class: 9
  poison_ratio: 0.4
poisoning_scheduler:
  name: 'interval'
  start_round: 1000
  end_round: 1100

# Defense
defense:
  name: 'krum'
  f: 1
  m: 0 # => Simple Krum

# Differential Privacy
differential_privacy:
  enabled: false
